{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98ed246",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Immigration Data\n",
    "\n",
    "This notebook demonstrates an ETL (Extract, Transform, Load) pipeline for immigration data. The pipeline includes:\n",
    "\n",
    "1. Adding new records to the full dataset\n",
    "2. Transforming the full dataset (enrichment, structural changes, categorization)\n",
    "3. Extracting and transforming only the latest (incremental) record\n",
    "4. Saving and displaying results as tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d30bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the dataset\n",
    "file_path =\"K:\\Code Projects\\Cloned Projects from jpchawanda1 Github\\ETL_Extract_Justice_Chawanda_670444\\Immigration_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic stats\n",
    "print(f\"Rows: {data.shape[0]}, Columns: {data.shape[1]}\")\n",
    "print(tabulate(data, headers='keys', tablefmt='grid'))\n",
    "\n",
    "# Print extraction message\n",
    "print(f\"Extracted {data.shape[0]} rows fully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c23f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Simulate a last extraction time\n",
    "last_extraction_file = \"last_extraction.txt\"\n",
    "if not os.path.exists(last_extraction_file):\n",
    "    with open(last_extraction_file, \"w\") as f:\n",
    "        f.write(\"2025-06-01 00:00:00\")  # Initial extraction time\n",
    "\n",
    "# Read the last extraction time\n",
    "with open(last_extraction_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read().strip()\n",
    "    try:\n",
    "        last_extraction_time = datetime.strptime(content, \"%Y-%m-%d %H:%M:%S\")\n",
    "    except ValueError:\n",
    "        # If the file content is invalid, reset to a default time\n",
    "        last_extraction_time = datetime(2025, 6, 1, 0, 0, 0)\n",
    "        with open(last_extraction_file, \"w\", encoding=\"utf-8\") as fw:\n",
    "            fw.write(last_extraction_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# Filter new or updated records using mixed datetime formats\n",
    "new_data = data[pd.to_datetime(data['timestamp'], format='mixed') > last_extraction_time]\n",
    "\n",
    "# Print the number of rows extracted incrementally\n",
    "print(f\"Extracted {new_data.shape[0]} rows incrementally since last check.\")\n",
    "\n",
    "# Update the last extraction time\n",
    "with open(last_extraction_file, \"w\") as f:\n",
    "    f.write(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# Display the last recorded timestamp\n",
    "if not data.empty:\n",
    "    last_timestamp = data['timestamp'].iloc[-1]\n",
    "    description = f\"The last record was added on: {last_timestamp}\"\n",
    "    # Write the description and last timestamp to the text file\n",
    "    with open(last_extraction_file, \"w\") as f:\n",
    "        f.write(description)\n",
    "else:\n",
    "    description = \"The dataset is empty.\"\n",
    "    with open(last_extraction_file, \"w\") as f:\n",
    "        f.write(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the user if they want to add a new record\n",
    "add_record = input(\"Do you want to add a new record? (yes/no): \").strip().lower()\n",
    "if add_record == 'yes':\n",
    "    # Collect new record details from the user\n",
    "    new_record = {\n",
    "        \"immigrant_id\": input(\"Enter immigrant ID: \"),\n",
    "        \"passport_number\": input(\"Enter passport number: \"),\n",
    "        \"name\": input(\"Enter name: \"),\n",
    "        \"country\": input(\"Enter country: \"),\n",
    "        \"purpose_of_visit\": input(\"Enter purpose of visit: \"),\n",
    "        \"contact\": input(\"Enter contact: \"),\n",
    "        \"payment_status\": input(\"Enter payment status: \"),\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "\n",
    "    # Append the new record to the dataset\n",
    "    data = pd.concat([data, pd.DataFrame([new_record])], ignore_index=True)\n",
    "\n",
    "    # Save the updated dataset back to the file\n",
    "    data.to_csv(file_path, index=False)\n",
    "\n",
    "    # Update the last_extraction.txt file with the description and timestamp of the very last record\n",
    "    last_timestamp = data['timestamp'].iloc[-1]\n",
    "    description = f\"The last record was added on: {last_timestamp}\"\n",
    "    with open(last_extraction_file, \"w\") as f:\n",
    "        f.write(description)\n",
    "\n",
    "    print(\"New record added successfully and timestamp updated!\")\n",
    "else:\n",
    "    print(\"No new record added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5dd86",
   "metadata": {},
   "source": [
    "## Section 4: Transform Full Data\n",
    "\n",
    "In this section, we will apply three transformations to the full dataset:\n",
    "1. **Enrichment**: Add a calculated 'age' column from the 'date_of_birth' column.\n",
    "2. **Structural**: Standardize the 'timestamp' column to always include seconds.\n",
    "3. **Categorization**: Bin records based on the 'country' column.\n",
    "\n",
    "The transformed data will be saved as `transformed_full.csv`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the full dataset (replace with your actual file path if needed)\n",
    "full_data = pd.read_csv(\"K:\\Code Projects\\Cloned Projects from jpchawanda1 Github\\ETL_Extract_Justice_Chawanda_670444\\Immigration_Data.csv\")  # Change 'full.csv' to your actual full dataset filename\n",
    "\n",
    "# Add a 'date_of_birth' column with random dates between 1970-01-01 and 2005-12-31 if not present\n",
    "if 'date_of_birth' not in full_data.columns:\n",
    "    np.random.seed(42)\n",
    "    start_date = pd.to_datetime('1970-01-01')\n",
    "    end_date = pd.to_datetime('2005-12-31')\n",
    "    num_rows = len(full_data)\n",
    "    random_days = np.random.randint(0, (end_date - start_date).days, num_rows)\n",
    "    full_data['date_of_birth'] = start_date + pd.to_timedelta(random_days, unit='D')  # keep as datetime64[ns]\n",
    "\n",
    "# Calculate 'age' from 'date_of_birth'\n",
    "today = pd.to_datetime('today')\n",
    "full_data['age'] = (today - pd.to_datetime(full_data['date_of_birth'])).dt.days // 365\n",
    "\n",
    "# Standardize 'timestamp' column to include seconds\n",
    "if 'timestamp' in full_data.columns:\n",
    "    full_data['timestamp'] = pd.to_datetime(full_data['timestamp'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Bin countries into continent-based regions\n",
    "def country_to_region(country):\n",
    "    africa = ['Nigeria', 'Kenya', 'South Africa', 'Egypt', 'Ghana']\n",
    "    europe = ['United Kingdom', 'France', 'Germany', 'Italy', 'Spain']\n",
    "    asia = ['China', 'India', 'Japan', 'Pakistan', 'Bangladesh']\n",
    "    north_america = ['United States', 'Canada', 'Mexico']\n",
    "    south_america = ['Brazil', 'Argentina', 'Colombia']\n",
    "    oceania = ['Australia', 'New Zealand']\n",
    "    if country in africa:\n",
    "        return 'Africa'\n",
    "    elif country in europe:\n",
    "        return 'Europe'\n",
    "    elif country in asia:\n",
    "        return 'Asia'\n",
    "    elif country in north_america:\n",
    "        return 'North America'\n",
    "    elif country in south_america:\n",
    "        return 'South America'\n",
    "    elif country in oceania:\n",
    "        return 'Oceania'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "if 'country' in full_data.columns:\n",
    "    full_data['country_group'] = full_data['country'].apply(country_to_region)\n",
    "\n",
    "# Save the transformed full dataset\n",
    "full_data.to_csv('transformed_full.csv', index=False)\n",
    "\n",
    "print('Added Date of Birth, calculated age, standardized timestamp, and grouped countries into regions for full transformation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564b2c9",
   "metadata": {},
   "source": [
    "\n",
    "## Section 5: Transform Incremental Data\n",
    "\n",
    "We will apply the same transformations to the incremental (new or updated) data and save the result as `transformed_incremental.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset (replace with your actual file path if needed)\n",
    "# Extract only the last (newest) record for incremental processing\n",
    "incremental_data = full_data.tail(1).copy()\n",
    "\n",
    "# 1. Enrichment: Add calculated 'age' from 'date_of_birth' column\n",
    "if 'date_of_birth' in incremental_data.columns:\n",
    "    today = pd.to_datetime('today')\n",
    "    incremental_data['date_of_birth'] = pd.to_datetime(incremental_data['date_of_birth'], errors='coerce')\n",
    "    incremental_data['age'] = (today - incremental_data['date_of_birth']).dt.days // 365\n",
    "\n",
    "# 2. Structural: Standardize 'timestamp' column to include seconds\n",
    "if 'timestamp' in incremental_data.columns:\n",
    "    incremental_data['timestamp'] = pd.to_datetime(incremental_data['timestamp'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# 3. Categorization: Bin records based on 'country'\n",
    "if 'country' in incremental_data.columns:\n",
    "    # Ensure access to the global country_bins variable\n",
    "    incremental_data['country_group'] = incremental_data['country'].map(globals().get('country_bins', {})).fillna('Other')\n",
    "\n",
    "# Save the transformed incremental dataset\n",
    "incremental_data.to_csv('transformed_incremental.csv', index=False)\n",
    "\n",
    "print('Transformed incremental dataset saved as transformed_incremental.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Add a 'date_of_birth' column with random dates between 1970-01-01 and 2005-12-31\n",
    "np.random.seed(42)\n",
    "start_date = pd.to_datetime('1970-01-01')\n",
    "end_date = pd.to_datetime('2005-12-31')\n",
    "num_rows = len(incremental_data)\n",
    "random_days = np.random.randint(0, (end_date - start_date).days, num_rows)\n",
    "incremental_data['date_of_birth'] = start_date + pd.to_timedelta(random_days, unit='D')  # keep as datetime64[ns]\n",
    "\n",
    "# Calculate 'age' from 'date_of_birth'\n",
    "today = pd.to_datetime('today')\n",
    "incremental_data['age'] = ((today - incremental_data['date_of_birth']).dt.days // 365).astype(int)\n",
    "\n",
    "# Bin countries into continent-based regions\n",
    "def country_to_region(country):\n",
    "    africa = ['Nigeria', 'Kenya', 'South Africa', 'Egypt', 'Ghana', 'Malawi', 'Zimbabwe']\n",
    "    europe = ['United Kingdom', 'France', 'Germany', 'Italy', 'Spain']\n",
    "    asia = ['China', 'India', 'Japan', 'Pakistan', 'Bangladesh']\n",
    "    north_america = ['United States', 'Canada', 'Mexico']\n",
    "    south_america = ['Brazil', 'Argentina', 'Colombia']\n",
    "    oceania = ['Australia', 'New Zealand']\n",
    "    if country in africa:\n",
    "        return 'Africa'\n",
    "    elif country in europe:\n",
    "        return 'Europe'\n",
    "    elif country in asia:\n",
    "        return 'Asia'\n",
    "    elif country in north_america:\n",
    "        return 'North America'\n",
    "    elif country in south_america:\n",
    "        return 'South America'\n",
    "    elif country in oceania:\n",
    "        return 'Oceania'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "if 'country' in incremental_data.columns:\n",
    "    incremental_data['country_group'] = incremental_data['country'].apply(country_to_region)\n",
    "\n",
    "print('Added Date of Birth, calculated age, standardized timestamp, and grouped countries into regions for incremental transformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedbb54",
   "metadata": {},
   "source": [
    "### Saving and Displaying Transformed Data\n",
    "\n",
    "This cell performs the following actions:\n",
    "\n",
    "- Saves the transformed `full_data` and `incremental_data` DataFrames to CSV files named `transformed_full.csv` and `transformed_incremental.csv`.\n",
    "- Prints confirmation messages indicating that the files have been saved.\n",
    "- Displays the first 10 rows of both the full and incremental datasets as formatted tables using the `tabulate` library for easy viewing.\n",
    "\n",
    "This step ensures that the results of the ETL process are both persisted and visually inspected for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89e8126e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full_data to transformed_full.csv.\n",
      "\n",
      "Full Data (first 10 rows):\n",
      "| immigrant_id   | passport_number   | name             | date_of_birth       |   age | contact                 | country       | country_group   | purpose_of_visit   | payment_status   | timestamp           |\n",
      "|----------------|-------------------|------------------|---------------------|-------|-------------------------|---------------|-----------------|--------------------|------------------|---------------------|\n",
      "| IM0094         | PPP45678910       | Anna Weber       | 1990-11-21 00:00:00 |    34 | anna.weber@email.de     | Germany       | Europe          | Tourism            | Paid             | 09-06-2025 18:52:00 |\n",
      "| IM0095         | QQQ56789021       | James O'Connor   | 2002-10-30 00:00:00 |    22 | james.oconnor@email.ie  | Ireland       | Other           | Study              | Paid             | 09-06-2025 21:08:00 |\n",
      "| IM0096         | RRR67890132       | Yusra Begum      | 1995-12-03 00:00:00 |    29 | yusra.begum@email.in    | India         | Asia            | Tourism            | Pending          | 09-06-2025 23:24:00 |\n",
      "| IM0097         | SSS78901243       | Laura Bianchi    | 1972-10-13 00:00:00 |    52 | laura.bianchi@email.it  | Italy         | Europe          | Business           | Paid             | 10-06-2025 01:40:00 |\n",
      "| IM0098         | TTT89012354       | Samuel Green     | 1991-07-19 00:00:00 |    33 | samuel.green@email.ca   | Canada        | North America   | Tourism            | Paid             | 10-06-2025 03:56:00 |\n",
      "| IM0099         | UUU90123465       | Ahmed Musa       | 1987-08-19 00:00:00 |    37 | ahmed.musa@email.ng     | Nigeria       | Africa          | Transit            | Pending          | 10-06-2025 06:12:00 |\n",
      "| IM0100         | VVV01234576       | Isabella Costa   | 1991-08-11 00:00:00 |    33 | isabella.costa@email.br | Brazil        | South America   | Study              | Paid             | 10-06-2025 08:28:00 |\n",
      "| IM0101         | BVB877123         | Pemba Comfort    | 1988-10-16 00:00:00 |    36 | comfort@email.com       | Malawi        | Other           | Study              | Paid             | 10-06-2025 21:35:00 |\n",
      "| IM0104         | HERK43853903      | Justice Chawanda | 1991-09-04 00:00:00 |    33 | chawanda@example.com    | Malawi        | Other           | Study              | Paid             | nan                 |\n",
      "| MI0103         | EWIO4359323       | Lucy Nkwinika    | 1993-05-09 00:00:00 |    32 | lucy@example.com        | United States | North America   | Study              | Paid             | nan                 |\n"
     ]
    }
   ],
   "source": [
    "# 4. Save and display the results as tables\n",
    "\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the desired column order\n",
    "column_order = ['immigrant_id','passport_number','name','date_of_birth','age','contact','country',\n",
    "                'country_group','purpose_of_visit','payment_status','timestamp']\n",
    "\n",
    "# Reorder columns for full_data and incremental_data if all columns exist\n",
    "def reorder_columns(df, order):\n",
    "    cols = [col for col in order if col in df.columns] + [col for col in df.columns if col not in order]\n",
    "    return df[cols]\n",
    "\n",
    "# Ensure timestamp is present and in the correct format for all records in full_data\n",
    "if 'timestamp' not in full_data.columns:\n",
    "    full_data['timestamp'] = np.nan\n",
    "full_data['timestamp'] = full_data['timestamp'].fillna(datetime.now().strftime('%d-%m-%Y %H:%M:%S'))\n",
    "full_data['timestamp'] = pd.to_datetime(full_data['timestamp'], errors='coerce').dt.strftime('%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "full_data = reorder_columns(full_data, column_order)\n",
    "\n",
    "full_data_fixed = (full_data.tail(10))\n",
    "\n",
    "# Save the updated DataFrames to CSV files\n",
    "full_data.to_csv('transformed_full.csv', index=False)\n",
    "\n",
    "print('Saved full_data to transformed_full.csv.')\n",
    "\n",
    "print('\\nFull Data (first 10 rows):')\n",
    "print(tabulate(full_data_fixed, headers='keys', tablefmt='github', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Save and display the results as tables\n",
    "\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the desired column order\n",
    "column_order = ['immigrant_id','passport_number','name','date_of_birth','age','contact','country',\n",
    "                'country_group','purpose_of_visit','payment_status','timestamp']\n",
    "\n",
    "# Reorder columns for full_data and incremental_data if all columns exist\n",
    "def reorder_columns(df, order):\n",
    "    cols = [col for col in order if col in df.columns] + [col for col in df.columns if col not in order]\n",
    "    return df[cols]\n",
    "\n",
    "incremental_data = reorder_columns(incremental_data, column_order)\n",
    "\n",
    "print('Saved incremental_data to transformed_incremental.csv')\n",
    "\n",
    "print('\\nIncremental Data (latest row):')\n",
    "incremental_data = (incremental_data.head(10))\n",
    "print(tabulate(incremental_data, headers='keys', tablefmt='github', showindex=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
